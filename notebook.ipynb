{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialect, Race, Gender, and The Pressure to Confirm to Standards\n",
    "\n",
    "## Introduction and Motivations\n",
    "Dialects are considered variations of languages that are generally mutually understandable. Due to English  being spoken in multiple different countries among diverse groups of speakers, English speakers exhibit diverse accents, localized words, and grammatical structures based on their countries and regions. Major English dialects are often classified into British Isles, North American, and Australasian categories. Dialects are not only linked to geographical regions but also to specific social groups. Each English-speaking country has its version of Standard English, often associated with education and formal communication.\n",
    "\n",
    "In this study, I examined and analyized how Microsoft's Speech-to-text translation API performs on different dialects, specifically standard American English, standard British English, and African American Venacular English, to see if the API's algorithm performs better on any of these groups compared to the others. I also wanted to see if this, or differences in race or differences in gender would yield a more drastic performance gap.\n",
    "\n",
    "## Dataset and Categories\n",
    "\n",
    "For the dataset, I collected the transcripts of SNL monologues from Youtube performed by different men and women, all of which were native English speakers. I downloaded the videos from Youtube from the website https://y2down.cc/en/youtube-wav.html. After getting the .wav files, I collected the transcripts from https://snltranscripts.jt.org/2022/megan-thee-stallion-monologue.phtml, then edited the transcript by listening to the files and making changes whereever needed. I also edited the .wav files, so it would only include the speaker's speech throughout the script, and edited out parts that had a lot of background noise (for example, clapping) to make ensure that the speech-to-text transcription algorithm is as fair between different speakers as possible.\n",
    "\n",
    "I had three different ways of analyzing performance as mentioned before:\n",
    "\n",
    "### Dialects\n",
    "First and foremost, I analyized the performance between dialects of English with speakers being categorized into one of three groups containing the following people:\n",
    "1. African American Venacular English:\n",
    "   - Keke Palmer (Black)\n",
    "   - Tiffany Haddish (Black)\n",
    "   - Eddie Murphy (Black)\n",
    "   - Megan Thee Stallion (Black)\n",
    "\n",
    "2. Standard American English\n",
    "   - Billie Eilish (white)\n",
    "   - Kim Kardashian (white)\n",
    "   - Don Cheadle (Black)\n",
    "   - Ayo Edebiri (Black)\n",
    "   - Jack Harlow (white)\n",
    "   - Pete Davidson (white)\n",
    "\n",
    "3. Standard British English\n",
    "   - Daniel Kaluuya (Black)\n",
    "   - Idris Elba (Black)\n",
    "   - Benedict Cumberbatch (white)\n",
    "   - Phoebe Waller-Bridge (white)\n",
    "\n",
    "### Race\n",
    "The second analysis measured how race affects the accuracy of speech-to-text translation, with two categories in this case:\n",
    "1. Black Speakers:\n",
    "   - Daniel Kaluuya \n",
    "   - Idris Elba \n",
    "   - Don Cheatle \n",
    "   - Keke Palmer \n",
    "   - Tiffany Haddish \n",
    "   - Eddie Murphy \n",
    "   - Megan Thee Stallion \n",
    "   - Ayo Edebiri \n",
    "  \n",
    "2. White Speakers:\n",
    "   - Benedict Cumberbatch \n",
    "   - Jack Harlow\n",
    "   - Pete Davidson\n",
    "   - Phoebe Waller-Bridge \n",
    "   - Billie Eilish \n",
    "   - Kim Kardashian \n",
    "\n",
    "### Gender\n",
    "I also wanted to see if gender would have a noticable impact on the performance of the speech-to-text algorithm, with categorizing the speakers into one of two categories:\n",
    "1. Women\n",
    "   - Keke Palmer \n",
    "   - Tiffany Haddish \n",
    "   - Megan Thee Stallion\n",
    "   - Ayo Edebiri\n",
    "   - Billie Eilish \n",
    "   - Kim Kardashian  \n",
    "   - Phoebe Waller-Bridge\n",
    "2. Men\n",
    "   - Benedict Cumberbatch\n",
    "   - Pete Davidson\n",
    "   - Jack Harlow\n",
    "   - Eddie Murphy \n",
    "   - Daniel Kaluuya \n",
    "   - Idris Elba\n",
    "\n",
    "## Hypotheses and Setup \n",
    "My hypotheses was that dialect will have the biggest impact, second will be gender, then out of these three race will result in the smallest difference between groups. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now onto testing the hypothese! You will need to make sure that everything runs correctly. The current API key and setup should be able to run from your computer, but if it is not, please follow the following steps detailed below. These steps are entirely taken from Microsoft's Learning page, from the model called \"Create your first Azure AI speech to text application\" (link: https://learn.microsoft.com/en-us/training/modules/create-your-first-speech-to-text-app/2-create-azure-cognitive-services-account).\n",
    "\n",
    "\n",
    "### Creating an Azure AI services account using the Azure portal\n",
    "The multi-service resource is listed under Azure AI services > Azure AI services multi-service account in the portal. To create a multi-service resource follow these instructions:\n",
    "\n",
    "1. Create an Azure [portal](https://login.microsoftonline.com/organizations/oauth2/v2.0/authorize?redirect_uri=https%3A%2F%2Fportal.azure.com%2Fsignin%2Findex%2F&response_type=code%20id_token&scope=https%3A%2F%2Fmanagement.core.windows.net%2F%2Fuser_impersonation%20openid%20email%20profile&state=OpenIdConnect.AuthenticationProperties%3DaAMwGPpStCDv_6lBa9SXZEQneerv_UpksUFLCtuQ4feE5_o1VZDabhpVwu3zHXio-O5EGebbdmepUg7IIgBb4puSw305JusVM6gfQ2mkuowUjyJ7ictMQyzEV_G_qfnNpzBbvA8tG0eRW_Mri7dVb6OIB-UiQB7gP52DgdtX6pWhnPGaUGZmniuz7Y5cqMDaItSWwz73U2nRJmjLM5yiAWBHS4o62xLGOaWLjS0uuDq8cjqbGIvKSyK0BU65rQuEONVb_lB3vkwh9ByT9TE17zVkS12HZOi5lMl340PXFUU3R1IcxHeCL4H-futF49RIuFbpwJM0gEsmuc03IZRLnfX317sjgInVWpqMyOCYxAtMkDQoTen5XtdbJ-jiAGELa4TFXJrY9hXPL69REYhrEFMbDJerC2j3svxdmvcf1QcOIszw5RYpTLUURHPgBU5oyFQUFfklj05ud_44uA-5ccXwSoGZ4dn_0cW5sguH7Dfm0dQufgmjYjlUNfFYHO1zyGk1sUzpZvSdxg1Nve0U7Gx1VEFc6qUbskqffUYxJM1tmpatUrHqS2kFLInhi8cyxZrUxeApNHOg_AICIvom8Q&response_mode=form_post&nonce=638449075930620669.MzJkZTBlNjEtZDk0Yi00MjUxLWJhOTUtZjg3YTg3MDJkNDIyYThmMjlmMTctNWYzZC00NDhmLTljODgtYWU4ODNiZjYwNDFi&client_id=c44b4083-3bb0-49c1-b47d-974e53cbdf3c&site_id=501430&client-request-id=32bf33d0-8e7a-4339-91a2-22de1b6998b4&x-client-SKU=ID_NET472&x-client-ver=6.34.0.0) if you don't already have one. You can use your Cornell email to create an account. If you already have an account, then sign in at the same link.\n",
    "\n",
    "2. Create a multi-service resource [here](https://portal.azure.com/#create/Microsoft.CognitiveServicesAllInOne)\n",
    "\n",
    "3. This should take you to a page titled \"Create Azure AI services\". Provide the information it asks for. \n",
    "\n",
    "4. After you are done, with the previous step, you should see a green checkmark and a confirmation saying \"Your deployment is complete\". If you see this, now click on \"Deployment details\"\n",
    "   \n",
    "5. Now, click on the link under \"Resource\" that should be named whatever you named it in step 3.\n",
    "   \n",
    "6. You should now see a subpage called \"Essentials\" in the middle of the screen. Click on the the link at \"Manage keys\". This should take you to a page called \"Keys and Endpoint\"\n",
    "   \n",
    "7. Once you are at the page \"Keys and Endpoint\"  copy \"KEY 1\". \n",
    "   \n",
    "8. Now open Program.cs. Locate line 25: string azureKey = \"c6330815003e4d7d94e03f17aa36a880\". replace the value of it with the value you copied from \"KEY 1\"\n",
    "  \n",
    "9. Now go back to the page \"Keys and Endpoint\" and copy the value at \"Location/Region\". \n",
    "    \n",
    "10. Now open Program.cs. Locate line 25: string azureLocation = \"eastus\";. Replace the value of it with the value you copied from \"Location/Region\"\n",
    "    \n",
    "11. Save your changes!\n",
    "    \n",
    "12. Now go back to [here](https://learn.microsoft.com/en-us/training/modules/create-your-first-speech-to-text-app/2-create-azure-cognitive-services-account). Go to unit 3 out of 8 by clicking the arrow.\n",
    "    \n",
    "13. Follow the directions in Unit 3 in the terminal of **THE JUPYTER NOTEBOOK**\n",
    "    \n",
    "14. After finishing Unit 3, you should be all set to continue to the next step, which is installing dependencies. If you run into any troubles, just continue onto unit 4 and follow links wherever needed.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies [DO THIS BEFORE RUNNING ANY CODE]\n",
    "\n",
    "To be able to run the algorithm you will need to install serval dependencies and set up a virtual environent. Follow these steps to ensure smooth running of the algorithm:\n",
    "\n",
    "1. Open your terminal in VS code by going to Terminal >> New Terminal and type `dotnet add package Microsoft.CognitiveServices.Speech`\n",
    "   \n",
    "2. Now, still in the terminal, type `code Program.cs` to ensure that the system is set up correctly. \n",
    "\n",
    "3. To be able to run C# code from a Jupyter Python kernel, ensure that you have .NET installed\n",
    "   To download do the following steps:\n",
    "   1. Go to https://dotnet.microsoft.com/en-us/download\n",
    "   2. Download the appropriate package for your operating system \n",
    "   3. To find the path of where .NET was installed open your terminal and do the following:\n",
    "      - type \"dotnet\" into your terminal to ensure that it downloaded correctly\n",
    "         it should output something like this:\n",
    "\n",
    "         `(base) yourname@dhcp-vl2041-25861 ~ % dotnet`\n",
    "\n",
    "         `Usage: dotnet [options]`\n",
    "\n",
    "         `Usage: dotnet [path-to-application]`\n",
    "\n",
    "         `Options:`\n",
    "\n",
    "         ` -h|--help         Display help.`\n",
    "\n",
    "         ` --info            Display .NET information.`\n",
    "\n",
    "         ` --list-sdks       Display the installed SDKs.`\n",
    "\n",
    "         ` --list-runtimes   Display the installed runtimes.`\n",
    "\n",
    "         `path-to-application:`\n",
    "\n",
    "         `The path to an application .dll file to execute.`\n",
    "\n",
    "      - If it's successful, type \"where dotnet\" into your terminal\n",
    "         That should output something like this:\n",
    "\n",
    "         \n",
    "         `(base) yourname@dhcp-vl2041-25861 ~ % where dotnet`\n",
    "\n",
    "        ` /usr/local/share/dotnet/dotnet`\n",
    "\n",
    "         Copy the line \"`/usr/local/share/dotnet/dotnet`\". This may be slightly different for you and that okay. In the code cell below (3 cells below) replace the line `dotnet_path = \"/usr/local/share/dotnet/dotnet\"` with your actual path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS: Running the following cell takes about 20-30 minutes depending on your computer. I prepopulated the files in case you (my TA or whoever is grading this:D) don't want to wait that long. If you want to double check that that this cell still runs as expected you can comment out the for loop and instead run the part currently commented out a few cells below. You can also replace the name with any name from the following list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ayo_edebiri', 'benedict_cumberbatch', 'billie_eilish', 'daniel_kaluuya', 'don_cheadle', 'eddie_murphy', 'idris_elba', 'jack_harlow', 'keke_palmer', 'kim_kardashian', 'megan_thee_stallion', 'pete_davidson', 'phoebe_waller_bridge', 'tiffany_haddish']\n"
     ]
    }
   ],
   "source": [
    "from helpers import name_info\n",
    "\n",
    "print(list(name_info.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textdistance in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.6.1)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (3.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install textdistance\n",
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import helpers \n",
    "speaker_list = list(helpers.name_info.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech recognition started for ayo_edebiri.\n",
      "Speech recognition stopped for ayo_edebiri.\n",
      "Speech recognition started for benedict_cumberbatch.\n",
      "Speech recognition stopped for benedict_cumberbatch.\n",
      "Speech recognition started for billie_eilish.\n",
      "Speech recognition stopped for billie_eilish.\n",
      "Speech recognition started for daniel_kaluuya.\n",
      "Speech recognition stopped for daniel_kaluuya.\n",
      "Speech recognition started for don_cheadle.\n",
      "Speech recognition stopped for don_cheadle.\n",
      "Speech recognition started for eddie_murphy.\n",
      "Speech recognition stopped for eddie_murphy.\n",
      "Speech recognition started for idris_elba.\n",
      "Speech recognition stopped for idris_elba.\n",
      "Speech recognition started for jack_harlow.\n",
      "Speech recognition stopped for jack_harlow.\n",
      "Speech recognition started for keke_palmer.\n",
      "Speech recognition stopped for keke_palmer.\n",
      "Speech recognition started for kim_kardashian.\n",
      "Speech recognition stopped for kim_kardashian.\n",
      "Speech recognition started for megan_thee_stallion.\n",
      "Speech recognition stopped for megan_thee_stallion.\n",
      "Speech recognition started for pete_davidson.\n",
      "Speech recognition stopped for pete_davidson.\n",
      "Speech recognition started for phoebe_waller_bridge.\n",
      "Speech recognition stopped for phoebe_waller_bridge.\n",
      "Speech recognition started for tiffany_haddish.\n",
      "Speech recognition stopped for tiffany_haddish.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import textdistance\n",
    "\n",
    "def run_speech_to_text(name):\n",
    "    current_directory = os.getcwd()\n",
    "    dotnet_path = \"/usr/local/share/dotnet/dotnet\"  # Replace with your actual path\n",
    "    subprocess.run([dotnet_path, \"run\", name], cwd=current_directory)\n",
    "\n",
    "# Example usage\n",
    "speaker_list = list(name_info.keys())\n",
    "\n",
    "for speaker in speaker_list:\n",
    "  run_speech_to_text(speaker)\n",
    "\n",
    "# This part can be un-commented if you don't want to wait the 22 minutes to rerun and instead just want to check one or a few people's transcripts. \n",
    "# run_speech_to_text(\"INSERT_YOUR_SPEAKER\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze how Microsoft's API performed, I wanted to measure the accuracy based on several factors. \n",
    "I did some minor pre-processing, like turning the text documents into strings, taking out punctuation, and turning all characters into lowercase letters to reduce translation mistakes that are lower than word level (for example, putting a different punctionation mark, or capitalizing letters differently).\n",
    "\n",
    "I used some predefined functions in Python's \"textdistance\" library (taken from https://pypi.org/project/textdistance/), as well as some code I wrote. To get an overall view on how the API performed on different demographics, I used 4 types measurements to calculate how it performed on each of them. \n",
    "\n",
    "These 4 types were:\n",
    "1. Edit based:\n",
    "   This measures how much post-translation editing a person would have to do get to the accurate transcript. The functions for this type were\n",
    "   - Accuracy: how many words the API got correctly --> closer to 1 is better, 0 is worse\n",
    "   - Word-error-rate: how much insertions/deletion/swaps to get to the correct transcript --> closer to 0 is better, 1 is worse\n",
    "2. Token based\n",
    "   - Cosine-similarity: calculates the similarity of two vectors by the dot product and divides it by the magnitudes of each vector (1 is better)\n",
    "   - Jaccard_distance: calculates the similarity of two text documents by comparing the number unique of terms used in both documents (1 is better)\n",
    "3. Compression based\n",
    "   Ð¢his is based on the idea that similar strings can be compressed more effectively than less similar ones ones.\n",
    "   - Square root: compares the size of compressed data (which is thesum of square roots of counts of every element) between the 2 texts (0 is better) \n",
    "4. Phonetic based\n",
    "   - Match Rating Approach (MRA): \"indexing of words by their pronunciation developed by Western Airlines in 1977 for the indexation and comparison of homophonous names\" (Moore, G B.; Kuhns, J L.; Treffzs, J L.; Montgomery, C A. (Feb 1, 1977))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textdistance in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.6.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.26.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install textdistance\n",
    "! pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import string\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def generate_scores(name):\n",
    "\n",
    "    with open(\"transcript_files/\" + name + \".txt\") as f:\n",
    "        reference = f.readlines()\n",
    "    with open(\"output_files/\" + name + \"_output.txt\") as f:\n",
    "        hypothesis = f.readlines()\n",
    "\n",
    "    reference_str = \"\"\n",
    "    for lines in reference:\n",
    "        reference_str += lines\n",
    "\n",
    "    hypothesis_str = \"\"\n",
    "    for lines in hypothesis:\n",
    "        hypothesis_str += lines\n",
    "\n",
    "    hypothesis = re.sub(r'[^\\w\\s]',\" \",hypothesis_str.lower())\n",
    "    reference = re.sub(r'[^\\w\\s]',\" \",reference_str.lower())\n",
    "    \n",
    "    # edit based\n",
    "    accuracy = get_accuracy(reference, hypothesis)\n",
    "    wer = get_wer(reference, hypothesis)\n",
    "    # token based \n",
    "    cos_sim = cosine_sim(reference, hypothesis)\n",
    "    jaccard_distance = textdistance.jaccard(reference, hypothesis)\n",
    "    # compression based\n",
    "    sqrt_dist = textdistance.sqrt_ncd(reference, hypothesis)\n",
    "    # phonetic based\n",
    "    mra_distance = textdistance.mra(reference, hypothesis)\n",
    "\n",
    "    return (accuracy, wer, cos_sim, jaccard_distance, sqrt_dist, mra_distance)\n",
    "\n",
    "\n",
    "def get_wer(reference, hypothesis):\n",
    "    ref_words = reference.split()\n",
    "    hyp_words = hypothesis.split()\n",
    "    d = np.zeros((len(ref_words) + 1, len(hyp_words) + 1))\n",
    "    for i in range(len(ref_words) + 1):\n",
    "        d[i, 0] = i\n",
    "    for j in range(len(hyp_words) + 1):\n",
    "        d[0, j] = j\n",
    "    for i in range(1, len(ref_words) + 1):\n",
    "        for j in range(1, len(hyp_words) + 1):\n",
    "            if ref_words[i - 1] == hyp_words[j - 1]:\n",
    "                d[i, j] = d[i - 1, j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1, j - 1] + 1\n",
    "                insertion = d[i, j - 1] + 1\n",
    "                deletion = d[i - 1, j] + 1\n",
    "                d[i, j] = min(substitution, insertion, deletion)\n",
    "\n",
    "    wer = d[len(ref_words), len(hyp_words)] / len(ref_words)\n",
    "    return wer\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    re.sub(r'[^\\w\\s]',\" \",text.lower())\n",
    "    text = text.split(\" \")\n",
    "    return text\n",
    "\n",
    "def cosine_sim(text1, text2):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=normalize)\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0, 1]\n",
    "\n",
    "\n",
    "def get_accuracy(reference, hypothesis):\n",
    "    ref_words = reference.split()\n",
    "    hyp_words = hypothesis.split()\n",
    "    \n",
    "    d = np.zeros((len(ref_words) + 1, len(hyp_words) + 1))\n",
    "    for i in range(len(ref_words) + 1):\n",
    "        d[i, 0] = i\n",
    "    for j in range(len(hyp_words) + 1):\n",
    "        d[0, j] = j\n",
    "    for i in range(1, len(ref_words) + 1):\n",
    "        for j in range(1, len(hyp_words) + 1):\n",
    "            if ref_words[i - 1] == hyp_words[j - 1]:\n",
    "                d[i, j] = d[i - 1, j - 1]\n",
    "            else:\n",
    "                d[i, j] = d[i - 1, j - 1] + 1\n",
    "\n",
    "    accuracy = d[len(ref_words), len(hyp_words)] / len(ref_words)\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from helpers import name_info\n",
    "# from analysis import generate_scores \n",
    "\n",
    "speaker_scores = {}\n",
    "for name in name_info.keys():\n",
    "  speaker_scores[name] = generate_scores(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Women: (0.989, 0.124, 0.98, 0.935, 0.422, 2.0)\n",
      "Men: (0.851, 0.105, 0.971, 0.961, 0.423, 1.714)\n",
      "\n",
      "SAE Speakers: (0.896, 0.077, 0.979, 0.959, 0.42, 1.8)\n",
      "SBE Speakers: (0.957, 0.128, 0.979, 0.959, 0.422, 1.833)\n",
      "AAVE Speakers: (0.887, 0.153, 0.962, 0.906, 0.43, 2.0)\n",
      "\n",
      "Black Speakers: (0.937, 0.147, 0.97, 0.938, 0.425, 1.714)\n",
      "White Speakers: (0.903, 0.083, 0.981, 0.958, 0.421, 2.0)\n"
     ]
    }
   ],
   "source": [
    "# sae_speakers, sbe_speakers, aave_speakers,female_speakers, male_speakers, black_speakers, white_speakers\n",
    "from helpers import name_info\n",
    "\n",
    "def extract_info_of_criteria(criteria, group):\n",
    "  #accuracy, wer, cos_sim, jaccard_distance, ratcliff_ob, sqrt_didt, mra_distance)\n",
    "  acc, wer, cos_sim, jacc_dist, sqrt_dist, mra_distance = 0, 0, 0, 0, 0, 0\n",
    "  num_in_group = 0\n",
    "\n",
    "  for speaker in speaker_scores:\n",
    "    if name_info[speaker][group] == criteria:\n",
    "      acc += speaker_scores[speaker][0]\n",
    "      wer += speaker_scores[speaker][1]\n",
    "      cos_sim += speaker_scores[speaker][2]\n",
    "      jacc_dist += speaker_scores[speaker][3]\n",
    "      sqrt_dist+=  speaker_scores[speaker][4]\n",
    "      mra_distance +=  speaker_scores[speaker][5]\n",
    "      num_in_group += 1\n",
    "    \n",
    "  return (round(acc/num_in_group, 3), \n",
    "          round(wer/num_in_group, 3), \n",
    "          round(cos_sim/num_in_group, 3),  \n",
    "          round(jacc_dist/num_in_group, 3),  \n",
    "          round(sqrt_dist/num_in_group, 3), \n",
    "          round(mra_distance/num_in_group, 3))\n",
    "\n",
    "score_of_men = extract_info_of_criteria(\"male\", \"gender\")\n",
    "score_of_women = extract_info_of_criteria(\"female\", \"gender\")\n",
    "\n",
    "score_of_sae = extract_info_of_criteria(\"sae\", \"dialect\")\n",
    "score_of_sbe = extract_info_of_criteria(\"sbe\", \"dialect\")\n",
    "score_of_aave = extract_info_of_criteria(\"aave\", \"dialect\")\n",
    "\n",
    "score_of_black_speakers = extract_info_of_criteria(\"black\", \"race\")\n",
    "score_of_white_speakers = extract_info_of_criteria(\"white\", \"race\")\n",
    "\n",
    "\n",
    "print(\"Women: \" + str(score_of_women))\n",
    "print(\"Men: \" + str(score_of_men))\n",
    "print()\n",
    "\n",
    "print(\"SAE Speakers: \" + str(score_of_sae))\n",
    "print(\"SBE Speakers: \" + str(score_of_sbe))\n",
    "print(\"AAVE Speakers: \" + str(score_of_aave))\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Black Speakers: \" + str(score_of_black_speakers))\n",
    "print(\"White Speakers: \" + str(score_of_white_speakers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since some of the measurements indicate high similarity when the score is 0, and some of them 1, I decided to flip the scores of the measurements that indicate high similarity when the score is 0 for the numbers to be easier to interpret. For the MRA, I decided to get the sum of all MRA scores for all demographics, then give the score of (1 - x/sum) for each demographic where x is their MRA score in order to normalize them. Therefore, the higher the modified MRA score the better the algorithm performed for that demographic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Women:\n",
      "[0.989, 0.876, 0.98, 0.935, 0.578, 0.847]\n",
      "\n",
      "Men:\n",
      "[0.851, 0.895, 0.971, 0.961, 0.577, 0.869]\n",
      "\n",
      "Standard American English Speakers:\n",
      "[0.896, 0.923, 0.979, 0.959, 0.58, 0.862]\n",
      "\n",
      "Standard British English Speakers:\n",
      "[0.957, 0.872, 0.979, 0.959, 0.578, 0.86]\n",
      "\n",
      "African American Vernacular English Speakers:\n",
      "[0.887, 0.847, 0.962, 0.906, 0.57, 0.847]\n",
      "\n",
      "Black Speakers:\n",
      "[0.937, 0.853, 0.97, 0.938, 0.575, 0.869]\n",
      "\n",
      "White Speakers:\n",
      "[0.903, 0.917, 0.981, 0.958, 0.579, 0.847]\n",
      "\n",
      "[[0.989, 0.876, 0.98, 0.935, 0.578, 0.847], [0.851, 0.895, 0.971, 0.961, 0.577, 0.869], [0.896, 0.923, 0.979, 0.959, 0.58, 0.862], [0.957, 0.872, 0.979, 0.959, 0.578, 0.86], [0.887, 0.847, 0.962, 0.906, 0.57, 0.847], [0.937, 0.853, 0.97, 0.938, 0.575, 0.869], [0.903, 0.917, 0.981, 0.958, 0.579, 0.847]]\n"
     ]
    }
   ],
   "source": [
    "# Closer to 0 is a better score: wer [1], square root distance [4]\n",
    "# Closer to 1 is a better score: accuracy [0], cosine similarity [2], jaccard distance [3]\n",
    "# The smaller the score the better: mra distance [5]\n",
    "reverse_index = {0: \"Women\", 1: \"Men\", 2: \"Standard American English Speakers\", 3:\"Standard British English Speakers\", 4:\"African American Vernacular English Speakers\", 5:\"Black Speakers\",  6:\"White Speakers\"}\n",
    "# We want to flip index 1 and index 4\n",
    "list_of_measurements = [score_of_women, score_of_men, score_of_sae, score_of_sbe, score_of_aave, score_of_black_speakers, score_of_white_speakers]\n",
    "list_of_measurements_modified = []\n",
    "\n",
    "mra_sum = sum(tup[5] for tup in list_of_measurements)\n",
    "\n",
    "for measurment in list_of_measurements:\n",
    "  measurment_mod = list(measurment)\n",
    "  measurment_mod[1] = (1 - measurment[1])\n",
    "  measurment_mod[4] = round((1 - measurment[4]), 3) \n",
    "  measurment_mod[5] = round((1 - (measurment[5]/mra_sum)), 3) \n",
    "  list_of_measurements_modified.append((measurment_mod))\n",
    "\n",
    "index = 0\n",
    "for line in list_of_measurements_modified:\n",
    "  print(reverse_index[index] + \":\")\n",
    "  print(str(line))\n",
    "  print()\n",
    "  index += 1\n",
    "print(list_of_measurements_modified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy Rankings:\n",
      "1: Women - 0.989\n",
      "2: Standard British English Speakers - 0.957\n",
      "3: Black Speakers - 0.937\n",
      "4: White Speakers - 0.903\n",
      "5: Standard American English Speakers - 0.896\n",
      "6: African American Vernacular English Speakers - 0.887\n",
      "7: Men - 0.851\n",
      "\n",
      "Word-Error-Rate Rankings:\n",
      "1: Standard American English Speakers - 0.923\n",
      "2: White Speakers - 0.917\n",
      "3: Men - 0.895\n",
      "4: Women - 0.876\n",
      "5: Standard British English Speakers - 0.872\n",
      "6: Black Speakers - 0.853\n",
      "7: African American Vernacular English Speakers - 0.847\n",
      "\n",
      "Cosine Similarity Rankings:\n",
      "1: White Speakers - 0.981\n",
      "2: Women - 0.98\n",
      "3: Standard American English Speakers - 0.979\n",
      "4: Standard British English Speakers - 0.979\n",
      "5: Men - 0.971\n",
      "6: Black Speakers - 0.97\n",
      "7: African American Vernacular English Speakers - 0.962\n",
      "\n",
      "Jaccard Distance Rankings:\n",
      "1: Men - 0.961\n",
      "2: Standard American English Speakers - 0.959\n",
      "3: Standard British English Speakers - 0.959\n",
      "4: White Speakers - 0.958\n",
      "5: Black Speakers - 0.938\n",
      "6: Women - 0.935\n",
      "7: African American Vernacular English Speakers - 0.906\n",
      "\n",
      "Square Root Distance Rankings:\n",
      "1: Standard American English Speakers - 0.58\n",
      "2: White Speakers - 0.579\n",
      "3: Women - 0.578\n",
      "4: Standard British English Speakers - 0.578\n",
      "5: Men - 0.577\n",
      "6: Black Speakers - 0.575\n",
      "7: African American Vernacular English Speakers - 0.57\n",
      "\n",
      "Match Rating Approach Rankings:\n",
      "1: Men - 0.869\n",
      "2: Black Speakers - 0.869\n",
      "3: Standard American English Speakers - 0.862\n",
      "4: Standard British English Speakers - 0.86\n",
      "5: Women - 0.847\n",
      "6: African American Vernacular English Speakers - 0.847\n",
      "7: White Speakers - 0.847\n"
     ]
    }
   ],
   "source": [
    "demographic_scores = np.array(list_of_measurements_modified)\n",
    "scores_demographics = demographic_scores.transpose()\n",
    "\n",
    "# Demographics labels\n",
    "demographics_labels = [\n",
    "    \"Women\", \"Men\", \"Standard American English Speakers\",\n",
    "    \"Standard British English Speakers\", \"African American Vernacular English Speakers\",\n",
    "    \"Black Speakers\", \"White Speakers\"\n",
    "]\n",
    "\n",
    "measurement =  [\"Accuracy\", \"Word-Error-Rate\", \"Cosine Similarity\", \"Jaccard Distance\", \"Square Root Distance\", \"Match Rating Approach\"]\n",
    "\n",
    "rankings = {}\n",
    "\n",
    "\n",
    "# Loop through each measurement\n",
    "index = 0\n",
    "for score in scores_demographics: \n",
    "    sorted_scores = sorted(score, reverse=True)\n",
    "    # sorted_indices = np.argsort(score)\n",
    "    # sorted_indices_rev = np.flip(sorted_indices)\n",
    "    meas_type_name = measurement[index]\n",
    "    meas_type = {}\n",
    "    for i in range(7):\n",
    "        demographic_index = np.where(score == sorted_scores[i])[0]\n",
    "        meas_type[i] = (reverse_index[i], score[i])\n",
    "\n",
    "    rankings[meas_type_name] = meas_type\n",
    "    index += 1\n",
    "\n",
    "\n",
    "# print(rankings)\n",
    "for measurement, values in rankings.items():\n",
    "    # Sort the values based on the key in reverse order\n",
    "    sorted_values = sorted(values.items(), key=lambda x: x[1][1], reverse=True)\n",
    "    # Print the sorted results\n",
    "    print(f\"\\n{measurement} Rankings:\")\n",
    "    for rank, (demographic, score) in enumerate(sorted_values, start=1):\n",
    "        print(f\"{rank}: {demographics_labels[demographic]} - {score[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Ranking | Accuracy | Word-Error-Rate | Cosine Similarity | Jaccard Distance | Square Root Difference | Match Rating Approach  \n",
    "|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n",
    "|   1   |   AAVE Speakers  |  SAE Speakers  |  SAE Speakers   | Men  | SAE Speakers  | Men   |\n",
    "|   2   |   Black Speakers  |  White Speakers |  White Speakers   |  SAE Speakers   | White Speakers   | SAE Speakers   |\n",
    "|   3   |   Women  |  Men   |  Men   | SBE Speakers   | Women   | SBE Speakers   |\n",
    "|   4   |   SBE Speakers  | Women   |  Women   | White Speakers   | SBE Speakers   | Black Speakers   |\n",
    "|   5   |   Men  |  SBE Speakers   |  SBE Speakers  | Black Speakers   | Men   | White Speakers   |\n",
    "|   6   |   White Speakers  | Black Speakers   |  AAVE Speakers  | Women  | Black Speakers  | Women   |\n",
    "|   7   |   SAE Speakers  |  AAVE Speakers   |   Black Speakers | AAVE Speakers   | AAVE Speakers   | AAVE Speakers  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While accuracy rankings highlight certain demographic disparities, it's crucial to note that accuracy alone doesn't effectively capture speech recognition performance. Recognizing individual words is just one aspect; understanding context and transcribing complete sentences accurately is equally important. Accuracy fails to account for errors in word order or contextual nuances.\n",
    "\n",
    "Notably, AAVE (African American Vernacular English) and Black speakers (often using AAVE) consistently perform poorly across metrics. Conversely, SAE (Standard American English) speakers, predominantly white, or white speakers individually, tend to rank higher.\n",
    "\n",
    "Even more concerningly, there is a significant drop in performance for AAVE speakers in the Match Rating Approach (MRA). This implies that both the speech-to-text algorithm and the MRA algorithm in Python's textdistance library do not adapt to diverse dialects' pronunciation variations. This oversight suggests a potential bias towards standard English, disregarding dialectal differences. This limitation hinders performance, even with diverse training data, as it fails to account for crucial grammatical distinctions in various dialects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for the sake of conciseness to have a singular number to rank all categories' overall performance instead of having to look at 6 different metrics, I then took the average score of all measurement types for each demographic, since all the results are now normalized and higher scores indicate better performance. It's important to note, however, that not all measurments carry the same weight, for example WER and MRA should not be given the same weight, but without formal research, it's hard to know what weights would capture the reality of the exact differences of the algorithm's performance. \n",
    "\n",
    "Also, I took out accuracy from this measurement, for the reasons mentioned above of it not being a good measurement system for real-time speech-to-text recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Standard American English Speakers 0.8606\n",
      "2. White Speakers 0.8564\n",
      "3. Men 0.8546\n",
      "4. Standard British English Speakers 0.8496\n",
      "5. Women 0.8432\n",
      "6. Black Speakers 0.841\n",
      "7. African American Vernacular English Speakers 0.8264\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "avg_scoring = {}\n",
    "idx = 0\n",
    "for demographic in list_of_measurements_modified:\n",
    "  demo = reverse_index[idx]\n",
    "  avg_scoring[demo] = round(np.mean(demographic[1:]), 5)\n",
    "  idx += 1\n",
    "\n",
    "sorted_scoring = dict(sorted(avg_scoring.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "place = 1\n",
    "for key, val in sorted_scoring.items():\n",
    "  print(str(place) + \". \" + key + \" \" + str(val))\n",
    "  place += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialects:\n",
      "Microsoft's API performed best on Standard American English of all categories, performing 0.011 points better than Standard British English.\n",
      "More concerningly, it performed 0.0342 better compared to African-American Vernacular English, which was also the worst performing demographic of all groups.\n",
      "\n",
      "Gender:\n",
      "Microsoft's API performed better on women speakers compared to men, scoring 0.0114 points better.\n",
      "\n",
      "Race:\n",
      "Microsoft's API performed better on white speakers compared to Black speakers, scoring 0.0154 points better.\n"
     ]
    }
   ],
   "source": [
    "diff_sae_and_sbe = round(sorted_scoring[\"Standard American English Speakers\"] - sorted_scoring[\"Standard British English Speakers\"], 5)\n",
    "diff_sae_and_aave = round(sorted_scoring[\"Standard American English Speakers\"] - sorted_scoring[\"African American Vernacular English Speakers\"], 5)\n",
    "\n",
    "diff_gender = round(sorted_scoring[\"Men\"] - sorted_scoring[\"Women\"], 5)\n",
    "\n",
    "diff_race = round(sorted_scoring[\"White Speakers\"] - sorted_scoring[\"Black Speakers\"], 5)\n",
    "\n",
    "print(\"Dialects:\")\n",
    "print(\"Microsoft's API performed best on Standard American English of all categories, performing \" + str(diff_sae_and_sbe) + \" points better than Standard British English.\")\n",
    "print(\"More concerningly, it performed \" + str(diff_sae_and_aave) + \" better compared to African-American Vernacular English, which was also the worst performing demographic of all groups.\")\n",
    "print()\n",
    "\n",
    "print(\"Gender:\")\n",
    "print(\"Microsoft's API performed better on women speakers compared to men, scoring \" + str(diff_gender) + \" points better.\")\n",
    "print()\n",
    "\n",
    "print(\"Race:\")\n",
    "print(\"Microsoft's API performed better on white speakers compared to Black speakers, scoring \" + str(diff_race) + \" points better.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, we can see that my original hypotheses was not entirely correct. I predicted that dialect will result in the largest disparity between the performance of the speech-to-text recognition, and this proved to be true. Standard British and American English performed significantly better than AAVE. Gender, however, had less of an impact on the performance of the algorithm, instead, race had a larger impact, with white speakers' speech being recognized 0.0214 better compared to Black speakers, while the algorithm performing only 0.0178 points better on mens' speeches compared to womens' speeches.\n",
    "\n",
    "It's important to note, however, that these findings might not be generally true for all kinds of speeches, and in all contexts, since in this study only examined the algorithm's performance on 14 speakers. A thorough, generalizable audit should include significantly more speakers (probably in the 100s, or even 1000s), and should also include differnt contexts, not just pre-written, well-perfomed SNL monologues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources, libraries, APIS used for this study:\n",
    "- https://en.wikipedia.org/wiki/List_of_dialects_of_English\n",
    "- https://dotnet.microsoft.com/en-us/download\n",
    "- https://learn.microsoft.com/en-us/training/modules/create-your-first-speech-to-text-app/2-create-azure-cognitive-services-account\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
